{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/tce/packages/python/python-3.7.2/lib/python3.7/site-packages/sklearn/ensemble/gradient_boosting.py:34: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  from ._gradient_boosting import predict_stages\n",
      "/usr/tce/packages/python/python-3.7.2/lib/python3.7/site-packages/sklearn/ensemble/gradient_boosting.py:34: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  from ._gradient_boosting import predict_stages\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import subprocess\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('/usr/workspace/vanover1/approx-llvm/approx')\n",
    "from approx_modules import approx\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to use /usr/tce/bin/git binary because the default git version grabbed by this subprocess is too early for the --show-superproject-working-tree flag\n",
    "REPO_ROOT = subprocess.check_output(\"/usr/tce/bin/git rev-parse --show-superproject-working-tree --show-toplevel | head -1\", shell=True).strip().decode()\n",
    "N_KERNELS = 10\n",
    "KERNEL_NAMES = [\n",
    "        'HPCCG::Ap_0',\n",
    "        'HPCCG::alpha_0',\n",
    "        'HPCCG::alpha_1',\n",
    "        'HPCCG::beta_0',\n",
    "        'HPCCG::normr_0',\n",
    "        'HPCCG::normr_1',\n",
    "        'HPCCG::r_0',\n",
    "        'HPCCG::x_0',\n",
    "        'main::x_0',\n",
    "        'main::b_0'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_HPCCG(nx, ny, nz):\n",
    "    \n",
    "    # compile and run HPCCG instrumented with HPAC directives\n",
    "    subprocess.check_call(f\"rm -f test.h5 && source {REPO_ROOT}/scripts/activate_env.sh && make && ./test_HPCCG {nx} {nx} {nz}\", shell=True)\n",
    "\n",
    "    # open database\n",
    "    approxDataProfile = approx.approxApplication(\"./test.h5\")\n",
    "    \n",
    "    # get output\n",
    "    Y = np.mean(approxDataProfile.getApplicationOutput()['solution'])\n",
    "    \n",
    "    # get aggregated kernel outputs\n",
    "    kernel_outs = []\n",
    "    for kernel_name in KERNEL_NAMES:\n",
    "        src_name = kernel_name.split(\"::\")[0]\n",
    "        var_name = kernel_name.split(\"::\")[-1]\n",
    "\n",
    "        for region_name in approxDataProfile.getRegionNames():\n",
    "            if region_name.startswith(src_name) and var_name in [region_name.split(\"::\")[-1].split(\",\")][0]:\n",
    "\n",
    "                if region_name == \"HPCCG::beta_0\":\n",
    "                    kernel_outs.append(approxDataProfile[region_name].X().mean())\n",
    "                elif region_name == \"main::x_0,b_0\":\n",
    "                    splitpoint = approxDataProfile[region_name].Y().shape[1]//2\n",
    "                    if var_name == 'x_0':\n",
    "                        kernel_outs.append(approxDataProfile[region_name].Y()[:,:splitpoint].mean())\n",
    "                    elif var_name == 'b_0':\n",
    "                        kernel_outs.append(approxDataProfile[region_name].Y()[:,splitpoint:].mean())\n",
    "                else:\n",
    "                    kernel_outs.append(approxDataProfile[region_name].Y().mean())\n",
    "                    \n",
    "    return kernel_outs, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample = 2**10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx = 20\n",
    "ny = 30\n",
    "nz = 160\n",
    "\n",
    "try:\n",
    "    with open(f\"XX_{n_sample}n.npy\", \"rb\") as f:\n",
    "        kernel_outs = np.load(f)\n",
    "    with open(f\"YY_{n_sample}n.npy\", \"rb\") as f:\n",
    "        YY = np.load(f)\n",
    "        \n",
    "except FileNotFoundError:\n",
    "\n",
    "    # run HPCCG\n",
    "    kernel_outs = []\n",
    "    YY = []\n",
    "    for n in range(n_sample):\n",
    "\n",
    "        temp = run_HPCCG(nx, ny, nz)\n",
    "        kernel_outs.append(temp[0])\n",
    "        YY.append(temp[1])\n",
    "\n",
    "    kernel_outs = np.array(kernel_outs)\n",
    "    YY = np.array(YY)\n",
    "    \n",
    "    with open(f\"XX_{n_sample}n.npy\", \"wb\") as f:\n",
    "        np.save(f, kernel_outs)\n",
    "    with open(f\"YY_{n_sample}n.npy\", \"wb\") as f:\n",
    "        np.save(f, YY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 819 samples.\n",
      "Score: 0.9685078068257224\n",
      "Sensitivities: [('HPCCG::alpha_0', 0.0), ('HPCCG::alpha_1', 0.0), ('HPCCG::beta_0', 0.0), ('HPCCG::normr_0', 0.0), ('HPCCG::r_0', 0.004583377093853409), ('HPCCG::Ap_0', 0.01683102053491955), ('main::b_0', 0.1087015570675255), ('HPCCG::normr_1', 0.18200707381872472), ('HPCCG::x_0', 0.31393760050084096), ('main::x_0', 0.3739393709841358)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/tce/packages/python/python-3.7.2/lib/python3.7/site-packages/sklearn/ensemble/base.py:158: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  dtype=np.int)\n"
     ]
    }
   ],
   "source": [
    "# train/test split\n",
    "split_proportion = 0.8\n",
    "split_point = int(n_sample * split_proportion)\n",
    "XX_train, XX_test = kernel_outs[:split_point], kernel_outs[split_point:]\n",
    "YY_train, YY_test = YY[:split_point], YY[split_point:]\n",
    "\n",
    "print(f\"Training with {split_point} samples.\")\n",
    "\n",
    "# analyze kernel output sensitivity\n",
    "et = ExtraTreesRegressor(n_estimators=100,\n",
    "                                    criterion=\"mse\",\n",
    "                                    # max_features=int(round(XX.shape[1] / 3)),\n",
    "                                    max_depth=8,\n",
    "                                    min_samples_split=2,\n",
    "                                    min_samples_leaf=max(1, int(round(np.sqrt(XX_train.shape[0]) / np.sqrt(1000)))),\n",
    "                                    min_weight_fraction_leaf=0,\n",
    "                                    max_leaf_nodes=None,\n",
    "                                    #bootstrap=True,\n",
    "                                    #oob_score=True,\n",
    "                                    random_state=1)\n",
    "\n",
    "et.fit(XX_train, YY_train)\n",
    "print(f\"Score: {et.score(XX_test, YY_test)}\")\n",
    "\n",
    "Si = [(KERNEL_NAMES[k_no], et.feature_importances_[k_no]) for k_no in range(N_KERNELS)]\n",
    "Si.sort(key = lambda x : x[1])\n",
    "print(f\"Sensitivities: {Si}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare to:\n",
    "```\n",
    "=== BEGIN ADAPT REPORT ===\n",
    "28704396 total independent/intermediate variables\n",
    "1 dependent variables\n",
    "Mixed-precision recommendation:\n",
    "  Replace variable normr:HPCCG.cpp:125      max error introduced: 0.00000e+00  count: 99          \n",
    "  Replace variable normr:HPCCG.cpp:105      max error introduced: 0.00000e+00  count: 1           \n",
    "  Replace variable x:main.cpp:181           max error introduced: 0.00000e+00  count: 96000       \n",
    "  Replace variable b:main.cpp:182           max error introduced: 0.00000e+00  count: 96000       \n",
    "  DO NOT replace   beta:HPCCG.cpp:120       max error introduced: 6.35086e-21  count: 98          \n",
    "  DO NOT replace   alpha:HPCCG.cpp:138      max error introduced: 3.59334e-20  count: 99          \n",
    "  DO NOT replace   alpha:HPCCG.cpp:137      max error introduced: 5.61582e-20  count: 99          \n",
    "  DO NOT replace   r:HPCCG.cpp:142          max error introduced: 2.05151e-08  count: 9504000     \n",
    "  DO NOT replace   Ap:HPCCG.cpp:135         max error introduced: 4.20565e-08  count: 9504000     \n",
    "  DO NOT replace   x:HPCCG.cpp:140          max error introduced: 1.85488e-07  count: 9504000     \n",
    "=== END ADAPT REPORT ===\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data for shaff analysis\n",
    "rows = []\n",
    "for n in range(n_sample):\n",
    "    row = {}\n",
    "    for i, kernel_name in enumerate(KERNEL_NAMES):\n",
    "        row[kernel_name] = kernel_outs[n][i]\n",
    "    row[\"Y\"] = YY[n]\n",
    "    rows.append(row)\n",
    "df = pd.DataFrame.from_dict(rows)\n",
    "df = df[[\"Y\"] + KERNEL_NAMES]\n",
    "df.to_csv(f\"df_{split_point}n.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From 5 SHAFF analysis runs with 204 samples:\n",
    "```\n",
    " [1] \"HPCCG..Ap_0\"    \"HPCCG..alpha_0\" \"HPCCG..alpha_1\" \"HPCCG..beta_0\" \n",
    " [5] \"HPCCG..normr_0\" \"HPCCG..normr_1\" \"HPCCG..r_0\"     \"HPCCG..x_0\"    \n",
    " [9] \"main..x_0\"      \"main..b_0\"     \n",
    " \n",
    " [1] 0.007311581 0.057866433 0.000000000 0.000000000 0.019603875 0.121547643\n",
    " [7] 0.000000000 0.315991220 0.301300586 0.163664125\n",
    " \n",
    " [1] 0.041131600 0.020491647 0.000000000 0.001783308 0.000000000 0.145431151\n",
    " [7] 0.034138667 0.301747721 0.326754565 0.115214073\n",
    " \n",
    " [1] 0.01231547 0.07257845 0.00000000 0.00000000 0.04976910 0.12349858\n",
    " [7] 0.01560563 0.31341909 0.24297095 0.15681582\n",
    " \n",
    " [1] 0.04108724 0.01305161 0.00000000 0.03102680 0.00745509 0.17842195\n",
    " [7] 0.03612008 0.23356233 0.29577123 0.15106809\n",
    " \n",
    " [1] 0.0313003424 0.0227614063 0.0338591616 0.0008722084 0.0584686394\n",
    " [6] 0.1197180796 0.0309788151 0.2998767624 0.2731233639 0.1160754408\n",
    "```\n",
    "\n",
    "Toss up between `HPCCG::x_0` and `main::x_0` followed by `main::b_0` and `HPCCG::normr_1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
